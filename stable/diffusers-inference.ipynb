{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steinhaug/stable-diffusion/blob/main/stable/diffusers-inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stable Diffusion - Diffusers\n",
        "\n",
        "Creating images with Stable Diffusion using the Diffusers library from Huggingface.  \n",
        "\n",
        "[![Buy me a beer](https://raw.githubusercontent.com/steinhaug/stable-diffusion/main/assets/buy-me-a-beer.png ) ](https://steinhaug.com/donate/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mG2vRmpIIzX",
        "outputId": "7472d433-f8dd-4be2-b2fb-5d2b30aa4f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla T4, 15360 MiB, 15101 MiB\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Check type of GPU and VRAM available, load notebook functions\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader\n",
        "\n",
        "import os\n",
        "\n",
        "use_gdrive = True #@param {type:\"boolean\"}\n",
        "\n",
        "if use_gdrive:\n",
        "    if not os.path.isdir('/content/drive/MyDrive'):\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "cellView": "form",
        "id": "eqZuG888lPrz"
      },
      "outputs": [],
      "source": [
        "#@markdown Load functions\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "_B=True\n",
        "_A=False\n",
        "def return__isValidDir(directory):\n",
        "    if os.path.isdir(directory):return _B\n",
        "    else:return _A\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows * cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows * cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "def decompress_zip(zip_file, dest_folder, flatten=False):\n",
        "    # Create the destination folder if it doesn't exist\n",
        "    if not os.path.exists(dest_folder):\n",
        "        os.makedirs(dest_folder)\n",
        "\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        # Extract all files into the destination folder\n",
        "        if flatten:\n",
        "            for file_info in zip_ref.infolist():\n",
        "                with zip_ref.open(file_info) as file, open(os.path.join(dest_folder, os.path.basename(file_info.filename)), 'wb') as output_file:\n",
        "                    output_file.write(file.read())\n",
        "        else:\n",
        "            zip_ref.extractall(dest_folder)\n",
        "\n",
        "\n",
        "def maxx(image_ref, max_width=256):\n",
        "    # Calculate the new height while maintaining the aspect ratio\n",
        "    aspect_ratio = image_ref.width / image_ref.height\n",
        "    new_height = int(max_width / aspect_ratio)\n",
        "\n",
        "    # Resize the image\n",
        "    image_ref.thumbnail((max_width, new_height))\n",
        "    return image_ref\n",
        "\n",
        "palette = np.asarray([\n",
        "    [0, 0, 0],\n",
        "    [120, 120, 120],\n",
        "    [180, 120, 120],\n",
        "    [6, 230, 230],\n",
        "    [80, 50, 50],\n",
        "    [4, 200, 3],\n",
        "    [120, 120, 80],\n",
        "    [140, 140, 140],\n",
        "    [204, 5, 255],\n",
        "    [230, 230, 230],\n",
        "    [4, 250, 7],\n",
        "    [224, 5, 255],\n",
        "    [235, 255, 7],\n",
        "    [150, 5, 61],\n",
        "    [120, 120, 70],\n",
        "    [8, 255, 51],\n",
        "    [255, 6, 82],\n",
        "    [143, 255, 140],\n",
        "    [204, 255, 4],\n",
        "    [255, 51, 7],\n",
        "    [204, 70, 3],\n",
        "    [0, 102, 200],\n",
        "    [61, 230, 250],\n",
        "    [255, 6, 51],\n",
        "    [11, 102, 255],\n",
        "    [255, 7, 71],\n",
        "    [255, 9, 224],\n",
        "    [9, 7, 230],\n",
        "    [220, 220, 220],\n",
        "    [255, 9, 92],\n",
        "    [112, 9, 255],\n",
        "    [8, 255, 214],\n",
        "    [7, 255, 224],\n",
        "    [255, 184, 6],\n",
        "    [10, 255, 71],\n",
        "    [255, 41, 10],\n",
        "    [7, 255, 255],\n",
        "    [224, 255, 8],\n",
        "    [102, 8, 255],\n",
        "    [255, 61, 6],\n",
        "    [255, 194, 7],\n",
        "    [255, 122, 8],\n",
        "    [0, 255, 20],\n",
        "    [255, 8, 41],\n",
        "    [255, 5, 153],\n",
        "    [6, 51, 255],\n",
        "    [235, 12, 255],\n",
        "    [160, 150, 20],\n",
        "    [0, 163, 255],\n",
        "    [140, 140, 140],\n",
        "    [250, 10, 15],\n",
        "    [20, 255, 0],\n",
        "    [31, 255, 0],\n",
        "    [255, 31, 0],\n",
        "    [255, 224, 0],\n",
        "    [153, 255, 0],\n",
        "    [0, 0, 255],\n",
        "    [255, 71, 0],\n",
        "    [0, 235, 255],\n",
        "    [0, 173, 255],\n",
        "    [31, 0, 255],\n",
        "    [11, 200, 200],\n",
        "    [255, 82, 0],\n",
        "    [0, 255, 245],\n",
        "    [0, 61, 255],\n",
        "    [0, 255, 112],\n",
        "    [0, 255, 133],\n",
        "    [255, 0, 0],\n",
        "    [255, 163, 0],\n",
        "    [255, 102, 0],\n",
        "    [194, 255, 0],\n",
        "    [0, 143, 255],\n",
        "    [51, 255, 0],\n",
        "    [0, 82, 255],\n",
        "    [0, 255, 41],\n",
        "    [0, 255, 173],\n",
        "    [10, 0, 255],\n",
        "    [173, 255, 0],\n",
        "    [0, 255, 153],\n",
        "    [255, 92, 0],\n",
        "    [255, 0, 255],\n",
        "    [255, 0, 245],\n",
        "    [255, 0, 102],\n",
        "    [255, 173, 0],\n",
        "    [255, 0, 20],\n",
        "    [255, 184, 184],\n",
        "    [0, 31, 255],\n",
        "    [0, 255, 61],\n",
        "    [0, 71, 255],\n",
        "    [255, 0, 204],\n",
        "    [0, 255, 194],\n",
        "    [0, 255, 82],\n",
        "    [0, 10, 255],\n",
        "    [0, 112, 255],\n",
        "    [51, 0, 255],\n",
        "    [0, 194, 255],\n",
        "    [0, 122, 255],\n",
        "    [0, 255, 163],\n",
        "    [255, 153, 0],\n",
        "    [0, 255, 10],\n",
        "    [255, 112, 0],\n",
        "    [143, 255, 0],\n",
        "    [82, 0, 255],\n",
        "    [163, 255, 0],\n",
        "    [255, 235, 0],\n",
        "    [8, 184, 170],\n",
        "    [133, 0, 255],\n",
        "    [0, 255, 92],\n",
        "    [184, 0, 255],\n",
        "    [255, 0, 31],\n",
        "    [0, 184, 255],\n",
        "    [0, 214, 255],\n",
        "    [255, 0, 112],\n",
        "    [92, 255, 0],\n",
        "    [0, 224, 255],\n",
        "    [112, 224, 255],\n",
        "    [70, 184, 160],\n",
        "    [163, 0, 255],\n",
        "    [153, 0, 255],\n",
        "    [71, 255, 0],\n",
        "    [255, 0, 163],\n",
        "    [255, 204, 0],\n",
        "    [255, 0, 143],\n",
        "    [0, 255, 235],\n",
        "    [133, 255, 0],\n",
        "    [255, 0, 235],\n",
        "    [245, 0, 255],\n",
        "    [255, 0, 122],\n",
        "    [255, 245, 0],\n",
        "    [10, 190, 212],\n",
        "    [214, 255, 0],\n",
        "    [0, 204, 255],\n",
        "    [20, 0, 255],\n",
        "    [255, 255, 0],\n",
        "    [0, 153, 255],\n",
        "    [0, 41, 255],\n",
        "    [0, 255, 204],\n",
        "    [41, 0, 255],\n",
        "    [41, 255, 0],\n",
        "    [173, 0, 255],\n",
        "    [0, 245, 255],\n",
        "    [71, 0, 255],\n",
        "    [122, 0, 255],\n",
        "    [0, 255, 184],\n",
        "    [0, 92, 255],\n",
        "    [184, 255, 0],\n",
        "    [0, 133, 255],\n",
        "    [255, 214, 0],\n",
        "    [25, 194, 194],\n",
        "    [102, 255, 0],\n",
        "    [92, 0, 255],\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4xSndBbFLHkU"
      },
      "outputs": [],
      "source": [
        "#@markdown Install dependencies\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "!pip install -q git+https://github.com/huggingface/accelerate.git\n",
        "!pip install git+https://github.com/huggingface/diffusers\n",
        "\n",
        "#!pip3 install Cython\n",
        "#!pip freeze | grep torch\n",
        "#!pip freeze | grep diffusers\n",
        "#!pip show torch | grep Version\n",
        "\n",
        "!wget --content-disposition https://t.co/9WZvWVGYqR -P /content\n",
        "!pip install /content/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl\n",
        "!pip install peft\n",
        "\n",
        "clear_output(); print(f\"Done (note you need to restart runtime if you get accellerate errors)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "krDfZiZfJWce"
      },
      "outputs": [],
      "source": [
        "# /content/drive/MyDrive/models/checkpoints/steinhaug/steinhaug.ckpt\n",
        "# /content/drive/MyDrive/models/checkpoints/steinhaug/kimsteinhaug-2000.ckpt\n",
        "# /content/drive/MyDrive/models/checkpoints/steinhaug/kimsteinhaug-3000.ckpt\n",
        "CKPT_FILEPATH = '/content/drive/MyDrive/models/checkpoints/steinhaug/mxmulti-5000.ckpt'\n",
        "cpu_or_cuda = 'cuda'\n",
        "\n",
        "model_path = '/content/drive/MyDrive/models/SD/sd1.5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wm_8rXAIWVp",
        "outputId": "213b8ee6-7e34-4cd9-c505-c7e37a061101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".ckpt file location: /content/drive/MyDrive/models/checkpoints/steinhaug/mxmulti-5000.ckpt\n"
          ]
        }
      ],
      "source": [
        "#@markdown 2.0: Initialize and load model\n",
        "sheduler_mode = \"EulerDiscrete\" # @param [\"DDIM\",\"UniPCMultistep\",\"DEISMultistep\",\"KDPM2Discrete\",\"EulerDiscrete\",\"PNDM\",\"HeunDiscrete\",\"EulerAncestralDiscrete\",\"DDPM\",\"KDPM2AncestralDiscrete\",\"LMSDiscrete\",\"DPMSolverSDE\",\"DPMSolverMultistep\",\"DPMSolverSinglestep\"]\n",
        "# pipe.scheduler.compatibles\n",
        "\n",
        "from IPython.display import display, clear_output\n",
        "import torch\n",
        "from torch import autocast\n",
        "from IPython.display import display\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "#pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(cpu_or_cuda)\n",
        "pipe = StableDiffusionPipeline.from_single_file(CKPT_FILEPATH, torch_dtype=torch.float16)\n",
        "pipe.to(cpu_or_cuda)\n",
        "\n",
        "if sheduler_mode == 'DDIM':\n",
        "    from diffusers import DDIMScheduler\n",
        "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'DDPM':\n",
        "    from diffusers import DDPMScheduler\n",
        "    pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'HeunDiscrete':\n",
        "    from diffusers import HeunDiscreteScheduler\n",
        "    pipe.scheduler = HeunDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'UniPCMultistep':\n",
        "    from diffusers import UniPCMultistepScheduler\n",
        "    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'EulerDiscrete':\n",
        "    from diffusers import EulerDiscreteScheduler\n",
        "    pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'EulerAncestralDiscrete':\n",
        "    from diffusers import EulerAncestralDiscreteScheduler\n",
        "    pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'KDPM2Discrete':\n",
        "    from diffusers import KDPM2DiscreteScheduler\n",
        "    pipe.scheduler = KDPM2DiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'KDPM2AncestralDiscrete':\n",
        "    from diffusers import KDPM2AncestralDiscreteScheduler\n",
        "    pipe.scheduler = KDPM2AncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'DPMSolverSDEScheduler':\n",
        "    from diffusers import DPMSolverSDEScheduler\n",
        "    pipe.scheduler = DPMSolverSDEScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'LMSDiscreteScheduler':\n",
        "    from diffusers import LMSDiscreteScheduler\n",
        "    pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'DPMSolverMultistepScheduler':\n",
        "    from diffusers import DPMSolverMultistepScheduler\n",
        "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'PNDM':\n",
        "    from diffusers import PNDMScheduler\n",
        "    pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'DEISMultistep':\n",
        "    from diffusers import DEISMultistepScheduler\n",
        "    pipe.scheduler = DEISMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'DPMSolverSinglestep':\n",
        "    from diffusers import DPMSolverSinglestepScheduler\n",
        "    pipe.scheduler = DPMSolverSinglestepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "g_cuda = None\n",
        "\n",
        "seed = 1\n",
        "g_cuda = torch.Generator(device=cpu_or_cuda)\n",
        "g_cuda.manual_seed(seed)\n",
        "saved_file_count = 1;\n",
        "\n",
        "clear_output();\n",
        "print(f'.ckpt file location: {CKPT_FILEPATH}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "cellView": "form",
        "id": "P4O6Y_23X8eI"
      },
      "outputs": [],
      "source": [
        "#@markdown 2.1: Optional, reselect the scheduler or/and set token\n",
        "#sheduler_mode = \"EulerDiscrete\" # @param [\"DDIM\",\"UniPCMultistep\",\"DEISMultistep\",\"KDPM2Discrete\",\"EulerDiscrete\",\"PNDM\",\"HeunDiscrete\",\"EulerAncestralDiscrete\",\"DDPM\",\"KDPM2AncestralDiscrete\",\"LMSDiscrete\",\"DPMSolverSDE\",\"DPMSolverMultistep\",\"DPMSolverSinglestep\"]\n",
        "sheduler_mode = \"EulerDiscrete\" # @param [\"EulerDiscrete\",\"DEISMultistep\",\"EulerAncestralDiscrete\",\"DPMSolverMultistep\"]\n",
        "\n",
        "YOUR_TOKEN = \"mxxstein\" #@param {type:\"string\"}\n",
        "TOKEN_GENDER = \"person\" #@param [\"person\", \"man\", \"woman\"]\n",
        "\n",
        "if sheduler_mode == 'DDIM':\n",
        "    from diffusers import DDIMScheduler\n",
        "    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'DDPM':\n",
        "    from diffusers import DDPMScheduler\n",
        "    pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'HeunDiscrete':\n",
        "    from diffusers import HeunDiscreteScheduler\n",
        "    pipe.scheduler = HeunDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'UniPCMultistep':\n",
        "    from diffusers import UniPCMultistepScheduler\n",
        "    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'EulerDiscrete':\n",
        "    from diffusers import EulerDiscreteScheduler\n",
        "    pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'EulerAncestralDiscrete':\n",
        "    from diffusers import EulerAncestralDiscreteScheduler\n",
        "    pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'KDPM2Discrete':\n",
        "    from diffusers import KDPM2DiscreteScheduler\n",
        "    pipe.scheduler = KDPM2DiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'KDPM2AncestralDiscrete':\n",
        "    from diffusers import KDPM2AncestralDiscreteScheduler\n",
        "    pipe.scheduler = KDPM2AncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'DPMSolverSDEScheduler':\n",
        "    from diffusers import DPMSolverSDEScheduler\n",
        "    pipe.scheduler = DPMSolverSDEScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'LMSDiscreteScheduler':\n",
        "    from diffusers import LMSDiscreteScheduler\n",
        "    pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'DPMSolverMultistepScheduler':\n",
        "    from diffusers import DPMSolverMultistepScheduler\n",
        "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'PNDM':\n",
        "    from diffusers import PNDMScheduler\n",
        "    pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'DEISMultistep':\n",
        "    from diffusers import DEISMultistepScheduler\n",
        "    pipe.scheduler = DEISMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "elif sheduler_mode == 'DPMSolverSinglestep':\n",
        "    from diffusers import DPMSolverSinglestepScheduler\n",
        "    pipe.scheduler = DPMSolverSinglestepScheduler.from_config(pipe.scheduler.config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yXNRGui3NcZE"
      },
      "outputs": [],
      "source": [
        "#@markdown 3.0: Generate image\n",
        "from slugify import slugify\n",
        "prompt = \"highest quality,award-winning,professional erotic photo of (sexy pale woman rancher with beautiful blonde hair),((pretty face)),seductive look,(wearing cowgirl outfit,daisy dukes),athletic body,huge breasts,large ass,candid,(breast worship:0.7),(detailed skin),[beauty marks],freckles,hyperrealistic photography,outdoors,standing in front of old western tavern,(storm clouds in the sky above),stormy weather,natural lighting,(crepuscular rays:0.6),photographed on a Canon EOS R6,50mm prime lens,F/1.4,(highly detailed),cinestill 800,8mm film grain,dynamic angle,full body,cowboy shot\" #@param {type:\"string\"}\n",
        "negative_prompt = \"child,childlike,(monochrome:1.3),(oversaturated:1.3),bad hands,lowers,3d render,cartoon,long body,((blurry)),duplicate,((duplicate body parts)),[wrong|anatomy],(disfigured),(poorly drawn),(extra limbs),fused fingers,extra fingers,(twisted),malformed hands,((((mutated hands and fingers)))),contorted,conjoined,((missing limbs)),logo,signature,text,words,low res,boring,mutated,artifacts,bad art,gross,ugly,poor quality,low quality,kid,missing asshole,\" #@param {type:\"string\"}\n",
        "token_name = YOUR_TOKEN + \" \" + TOKEN_GENDER + \", \" + YOUR_TOKEN\n",
        "num_samples = 1 #@ param {type:\"number\"}\n",
        "guidance_scale = 8 #@ param {type:\"number\"}\n",
        "num_inference_steps = 23 #@param {type:\"number\"}\n",
        "width = \"512\" #@ param [\"512\", \"768\", \"1280\", \"1536\"] {allow-input: true}\n",
        "height = \"768\" #@ param [\"512\", \"768\", \"1280\", \"1536\"] {allow-input: true}\n",
        "width = int(width)\n",
        "height = int(height)\n",
        "custom_seed = 2616847730 #@param {type:\"number\"}\n",
        "new_seed = custom_seed\n",
        "save_images_path = \"/content/drive/MyDrive/AI-Images-Manual\"\n",
        "\n",
        "#prompt = prompt.replace(\"__token__\", token_name)\n",
        "\n",
        "x = token_name.split(\",\")\n",
        "for index, value in enumerate(x):\n",
        "    if index == 0:\n",
        "        prompt = prompt.replace(\"__token__\", value)\n",
        "    else:\n",
        "        prompt = prompt.replace(\"__token\" + str(index) + \"__\", value)\n",
        "\n",
        "#raise Exception(1);\n",
        "\n",
        "if num_samples > 1:\n",
        "    print(\"You may need to scroll to see the rest of the images as they appear...\")\n",
        "\n",
        "if new_seed:\n",
        "    g_cuda = torch.Generator(device='cuda')\n",
        "    g_cuda.manual_seed(new_seed)\n",
        "\n",
        "if len(save_images_path):\n",
        "    tmp = save_images_path.split(\"/\")\n",
        "    if len(tmp) == 1:\n",
        "        save_images_path = \"/content/\" + save_images_path\n",
        "    from pathlib import Path\n",
        "    path = Path(save_images_path)\n",
        "    if not path.exists():\n",
        "        print(f\"[*] Create save directory...\")\n",
        "        path.mkdir(parents = False, exist_ok = False)\n",
        "    try:\n",
        "        if not image_save_count:\n",
        "            print('Darn we need this one!')\n",
        "    except NameError:\n",
        "        image_save_count = 1\n",
        "\n",
        "print(f\"[*] Prompt used: {prompt}\")\n",
        "\n",
        "gen_images = []\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    gen_images.append(img.copy())\n",
        "    display(img)\n",
        "    if len(save_images_path):\n",
        "        precount = f'{image_save_count:04d}'\n",
        "        image_filename = slugify(precount + '_' + prompt.replace(\" \", '_')[:240]) + '.png'\n",
        "        img.save(save_images_path + \"/\" + image_filename)\n",
        "        image_save_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7ENVPQrFb2AX"
      },
      "outputs": [],
      "source": [
        "#@markdown 3.1: continue, generating with array of schedulers, from previous block\n",
        "\n",
        "sheduler_list = [\n",
        "    \"EulerDiscrete\",\"DDIM\",\"UniPCMultistep\",\"DEISMultistep\",\"KDPM2Discrete\",\n",
        "    \"PNDM\",\"HeunDiscrete\",\"EulerAncestralDiscrete\",\"DDPM\",\n",
        "    \"KDPM2AncestralDiscrete\",\"LMSDiscrete\",\n",
        "    \"DPMSolverSDE\",\"DPMSolverMultistep\",\"DPMSolverSinglestep\"\n",
        "]\n",
        "sheduler_list = [\"EulerDiscrete\",\"DEISMultistep\",\"EulerAncestralDiscrete\",\"DPMSolverMultistep\"]\n",
        "\n",
        "from slugify import slugify\n",
        "num_samples = 1 #@ param {type:\"number\"}\n",
        "guidance_scale = 8 #@param {type:\"number\"}\n",
        "num_inference_steps = 23 #@param {type:\"number\"}\n",
        "width = \"512\" #@param [\"512\", \"768\", \"1280\", \"1536\"] {allow-input: true}\n",
        "height = \"768\" #@param [\"512\", \"768\", \"1280\", \"1536\"] {allow-input: true}\n",
        "width = int(width)\n",
        "height = int(height)\n",
        "custom_seed = 2616847730 #@param {type:\"number\"}\n",
        "new_seed = custom_seed\n",
        "save_images_path = \"/content/drive/MyDrive/AI-Images-Manual\"\n",
        "\n",
        "\n",
        "if num_samples > 1:\n",
        "    print(\"You may need to scroll to see the rest of the images as they appear...\")\n",
        "\n",
        "if len(save_images_path):\n",
        "    tmp = save_images_path.split(\"/\")\n",
        "    if len(tmp) == 1:\n",
        "        save_images_path = \"/content/\" + save_images_path\n",
        "    from pathlib import Path\n",
        "    path = Path(save_images_path)\n",
        "    if not path.exists():\n",
        "        print(f\"[*] Create save directory...\")\n",
        "        path.mkdir(parents = False, exist_ok = False)\n",
        "    try:\n",
        "        if not image_save_count:\n",
        "            print('Darn we need this one!')\n",
        "    except NameError:\n",
        "        image_save_count = 1\n",
        "\n",
        "print(f\"[*] Prompt used: {prompt}\")\n",
        "\n",
        "gen_images = []\n",
        "\n",
        "for sheduler_mode in sheduler_list:\n",
        "    if sheduler_mode == 'DDIM':\n",
        "        from diffusers import DDIMScheduler\n",
        "        pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'DDPM':\n",
        "        from diffusers import DDPMScheduler\n",
        "        pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'HeunDiscrete':\n",
        "        from diffusers import HeunDiscreteScheduler\n",
        "        pipe.scheduler = HeunDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'UniPCMultistep':\n",
        "        from diffusers import UniPCMultistepScheduler\n",
        "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'EulerDiscrete':\n",
        "        from diffusers import EulerDiscreteScheduler\n",
        "        pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'EulerAncestralDiscrete':\n",
        "        from diffusers import EulerAncestralDiscreteScheduler\n",
        "        pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'KDPM2Discrete':\n",
        "        from diffusers import KDPM2DiscreteScheduler\n",
        "        pipe.scheduler = KDPM2DiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'KDPM2AncestralDiscrete':\n",
        "        from diffusers import KDPM2AncestralDiscreteScheduler\n",
        "        pipe.scheduler = KDPM2AncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'DPMSolverSDEScheduler':\n",
        "        from diffusers import DPMSolverSDEScheduler\n",
        "        pipe.scheduler = DPMSolverSDEScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'LMSDiscreteScheduler':\n",
        "        from diffusers import LMSDiscreteScheduler\n",
        "        pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'DPMSolverMultistepScheduler':\n",
        "        from diffusers import DPMSolverMultistepScheduler\n",
        "        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'PNDM':\n",
        "        from diffusers import PNDMScheduler\n",
        "        pipe.scheduler = PNDMScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'DEISMultistep':\n",
        "        from diffusers import DEISMultistepScheduler\n",
        "        pipe.scheduler = DEISMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "    elif sheduler_mode == 'DPMSolverSinglestep':\n",
        "        from diffusers import DPMSolverSinglestepScheduler\n",
        "        pipe.scheduler = DPMSolverSinglestepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    if new_seed:\n",
        "        g_cuda = torch.Generator(device=cpu_or_cuda)\n",
        "        g_cuda.manual_seed(new_seed)\n",
        "\n",
        "    print(\" \")\n",
        "    print(f\"MODE: {sheduler_mode}\")\n",
        "    with autocast(\"cuda\"), torch.inference_mode():\n",
        "        images = pipe(\n",
        "            prompt,\n",
        "            height=height,\n",
        "            width=width,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_images_per_prompt=num_samples,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=g_cuda\n",
        "        ).images\n",
        "\n",
        "    for img in images:\n",
        "        gen_images.append(img.copy())\n",
        "        display(img)\n",
        "        if len(save_images_path):\n",
        "            precount = f'{image_save_count:04d}'\n",
        "            image_filename = slugify(precount + '_' + prompt.replace(\" \", '_')[:240]) + '.png'\n",
        "            img.save(save_images_path + \"/\" + image_filename)\n",
        "            image_save_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8xL9zVyiVUD"
      },
      "outputs": [],
      "source": [
        "print(\"50 steps\")\n",
        "image_grid(gen_images, 2, 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOfEgLD-nlSf"
      },
      "outputs": [],
      "source": [
        "print(\"20 steps\")\n",
        "image_grid(gen_images, 2, 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "w1gAQ_5PUyKQ"
      },
      "outputs": [],
      "source": [
        "#@markdown **AI-Manual - All options mode**\n",
        "from slugify import slugify\n",
        "prompt = \"mxxstein person\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "token_name = YOUR_TOKEN + \" \" + TOKEN_GENDER + \", \" + YOUR_TOKEN\n",
        "num_samples = 4 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5\n",
        "num_inference_steps = 30 #@param {type:\"number\"}\n",
        "width = \"512\"\n",
        "height = \"512\" #@param [\"512\", \"768\", \"1280\", \"1536\"] {allow-input: true}\n",
        "width = int(width)\n",
        "height = int(height)\n",
        "custom_seed = None\n",
        "new_seed = custom_seed\n",
        "save_images_path = \"/content/drive/MyDrive/AI-Images-Manual\"\n",
        "\n",
        "#prompt = prompt.replace(\"__token__\", token_name)\n",
        "\n",
        "x = token_name.split(\",\")\n",
        "for index, value in enumerate(x):\n",
        "    if index == 0:\n",
        "        prompt = prompt.replace(\"__token__\", value)\n",
        "    else:\n",
        "        prompt = prompt.replace(\"__token\" + str(index) + \"__\", value)\n",
        "\n",
        "#raise Exception(1);\n",
        "\n",
        "if num_samples > 1:\n",
        "    print(\"You may need to scroll to see the rest of the images as they appear...\")\n",
        "\n",
        "if new_seed:\n",
        "    g_cuda = torch.Generator(device='cuda')\n",
        "    g_cuda.manual_seed(new_seed)\n",
        "\n",
        "if len(save_images_path):\n",
        "    tmp = save_images_path.split(\"/\")\n",
        "    if len(tmp) == 1:\n",
        "        save_images_path = \"/content/\" + save_images_path\n",
        "    from pathlib import Path\n",
        "    path = Path(save_images_path)\n",
        "    if not path.exists():\n",
        "        print(f\"[*] Create save directory...\")\n",
        "        path.mkdir(parents = False, exist_ok = False)\n",
        "    try:\n",
        "        if not image_save_count:\n",
        "            print('Darn we need this one!')\n",
        "    except NameError:\n",
        "        image_save_count = 1\n",
        "\n",
        "print(f\"[*] Prompt used: {prompt}\")\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)\n",
        "    if len(save_images_path):\n",
        "        precount = f'{image_save_count:04d}'\n",
        "        image_filename = slugify(precount + '_' + prompt.replace(\" \", '_')[:240]) + '.png'\n",
        "        img.save(save_images_path + \"/\" + image_filename)\n",
        "        image_save_count += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOhiLsFM+AVNVHBkqEBSp+u",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
