{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1lJdmsXjDkHgoQ0DB1SUX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steinhaug/stable-diffusion/blob/main/tool/batch_depth_frames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Extract all frames from videofile,  \n",
        "2. crop frames to 768x512  \n",
        "3. create depth frames from frames"
      ],
      "metadata": {
        "id": "7NC9Fv5ZGIH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "from IPython.display import display, clear_output\n",
        "!apt -y update -qq\n",
        "!apt -y install -qq aria2\n",
        "!pip install timm\n",
        "clear_output();print('Done!')"
      ],
      "metadata": {
        "id": "6lpK29By2wST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Notebook functions\n",
        "\n",
        "import os\n",
        "_B=True\n",
        "_A=False\n",
        "def return__isValidDir(directory):\n",
        "    if os.path.isdir(directory):return _B\n",
        "    else:return _A"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9O8Wcn81Co9S"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_file = \"/content/video1.mp4\"\n",
        "frames_directory = \"/content/frames/video1\""
      ],
      "metadata": {
        "id": "XdMWAaOmDoZE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Xtract frames and crop frames"
      ],
      "metadata": {
        "id": "wiYS3SSe-nBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title . 1.1 Extract frames\n",
        "if not return__isValidDir(frames_directory):\n",
        "    os.makedirs(frames_directory)\n",
        "\n",
        "!ffmpeg -i {video_file} -vf \"scale=910:512\" {frames_directory}/c01_%04d.png\n",
        "\n",
        "clear_output();print(f'Frames extracted into {frames_directory}')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D-TbjaXT-qUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title . 1.2 Crop frames\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def process_images(input_directory, output_directory, target_width=768, target_height=512):\n",
        "    # Create the output directory if it doesn't exist\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "\n",
        "    # Get a list of all files in the input directory\n",
        "    image_files = [f for f in os.listdir(input_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        input_path = os.path.join(input_directory, image_file)\n",
        "        output_path = os.path.join(output_directory, image_file)\n",
        "\n",
        "        # Open the image\n",
        "        with Image.open(input_path) as img:\n",
        "            # Get the original image dimensions\n",
        "            original_width, original_height = img.size\n",
        "\n",
        "            # Calculate cropping dimensions to maintain aspect ratio and center the crop\n",
        "            left = max(0, (original_width - target_width) // 2)\n",
        "            top = max(0, (original_height - target_height) // 2)\n",
        "            right = min(original_width, left + target_width)\n",
        "            bottom = min(original_height, top + target_height)\n",
        "\n",
        "            # Crop the image\n",
        "            cropped_img = img.crop((left, top, right, bottom))\n",
        "\n",
        "            # Save the cropped image\n",
        "            cropped_img.save(output_path)\n",
        "\n",
        "process_images(frames_directory, frames_directory)\n",
        "\n",
        "clear_output();print(f'Frames cropped into 768x512')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "01P-n5a1DUgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Create depth maps"
      ],
      "metadata": {
        "id": "_ZA7R2RG-hMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title . 2.1 Initialise MiDaS depth model\n",
        "import torch\n",
        "\n",
        "model_type = \"DPT_Large\"     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)\n",
        "#model_type = \"DPT_Hybrid\"   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)\n",
        "#model_type = \"MiDaS_small\"  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)\n",
        "\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
        "clear_output();print('Depth model downloaded!')\n",
        "\n",
        "from IPython.display import display, clear_output\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "midas.to(device)\n",
        "midas.eval()\n",
        "\n",
        "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
        "    transform = midas_transforms.dpt_transform\n",
        "else:\n",
        "    transform = midas_transforms.small_transform\n",
        "\n",
        "clear_output();print('Model loaded and ready...')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qPNnEWz341wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title . 2.2 Process frames\n",
        "import cv2, os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def process_images(input_directory, output_directory):\n",
        "    # Create the output directory if it doesn't exist\n",
        "    if not os.path.exists(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "\n",
        "    # Get a list of all files in the input directory\n",
        "    image_files = [f for f in os.listdir(input_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        input_path = os.path.join(input_directory, image_file)\n",
        "        output_path = os.path.join(output_directory, image_file)\n",
        "\n",
        "\n",
        "        img = cv2.imread(input_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        input_batch = transform(img).to(device)\n",
        "        with torch.no_grad():\n",
        "            prediction = midas(input_batch)\n",
        "            prediction = torch.nn.functional.interpolate(\n",
        "                prediction.unsqueeze(1),\n",
        "                size=img.shape[:2],\n",
        "                mode=\"bicubic\",\n",
        "                align_corners=False,\n",
        "            ).squeeze()\n",
        "        output = prediction.cpu().numpy()\n",
        "        output_image = Image.fromarray(output.astype('uint8'))\n",
        "        output_image.save(output_path)\n",
        "\n",
        "process_images(frames_directory, f\"{frames_directory}_depth\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qnIMWtpT89bs"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}