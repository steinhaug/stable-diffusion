{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steinhaug/stable-diffusion/blob/main/steinhaug_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wJYg6DovNp8"
      },
      "source": [
        "_Version: v1.3_  \n",
        "___PS: you might want this one instead___   \n",
        "\n",
        "[![Open in Colab](https://img.shields.io/badge/Dreambooth%20for%20people%20in%20a%20hurry-Open%20in%20Colab-blue?logo=google-colab)](https://colab.research.google.com/github/steinhaug/stable-diffusion/blob/main/Dreambooth_Colab_edition_for_people_in_a_hurry_fp16.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU7NuMAA2drw",
        "outputId": "eace2c5a-657d-4390-d60b-2685603ab356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla T4, 15360 MiB, 15101 MiB\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Connect runtime and mount Google Drive\n",
        "#@markdown Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader\n",
        "\n",
        "save_to_gdrive = True #@param {type:\"boolean\"}\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLWXPZqjsZVV",
        "outputId": "259e370b-46a3-4923-e008-f1292f861230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;32mAll dependencies installed! âœ“\n"
          ]
        }
      ],
      "source": [
        "#@title Install dependencies\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py -O train_dreambooth_new.py\n",
        "!wget https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_original_stable_diffusion_to_diffusers.py -O convert_original_stable_diffusion_to_diffusers.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers\n",
        "%pip install omegaconf\n",
        "\n",
        "clear_output()\n",
        "print('\u001b[1;32mAll dependencies installed! âœ“')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4lqqWT_uxD2",
        "outputId": "2d5b6596-5ee2-4be4-81ee-d59677caaf68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;32mDone! âœ“\n"
          ]
        }
      ],
      "source": [
        "#@title Login to HuggingFace ðŸ¤—\n",
        "#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree. You have to be a registered user in ðŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work.\n",
        "\n",
        "!mkdir -p ~/.cache\n",
        "!mkdir -p ~/.cache/huggingface\n",
        "HUGGINGFACE_TOKEN = \"\" #@param {type:\"string\"}\n",
        "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.cache/huggingface/token\n",
        "\n",
        "clear_output()\n",
        "print('\u001b[1;32mDone! âœ“')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxg0y5MBudmd",
        "outputId": "deca6330-e419-441b-aaf6-8f103b95597a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;32mDone! âœ“\n",
            "[*] Weights will be saved at /content/drive/MyDrive/sd_weights/steinhaug-big-realvision50n\n"
          ]
        }
      ],
      "source": [
        "#@title Select model and define folder for save\n",
        "#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "from IPython.display import clear_output;\n",
        "save_to_gdrive = True #@param {type:\"boolean\"}\n",
        "\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Name/Path of the initial model.\n",
        "MODEL_NAME = \"SG161222/Realistic_Vision_V5.0_noVAE\" #@param [\"sd-dreambooth-library/disco-diffusion-style\", \"runwayml/stable-diffusion-v1-5\", \"dreamlike-art/dreamlike-photoreal-2.0\", \"SG161222/Realistic_Vision_V2.0\", \"SG161222/Realistic_Vision_V5.0_noVAE\", \"prompthero/openjourney\"] {allow-input: true}\n",
        "#, \"Lykon/DreamShaper\"\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "OUTPUT_DIR = \"sd_weights/steinhaug-big-realvision50n\" #@param {type:\"string\"}\n",
        "\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR\n",
        "\n",
        "!mkdir /content/data/\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# - - -\n",
        "# Copy n files from src to dst\n",
        "# - - -\n",
        "# Example: copy_nfiles(src,dst,500)\n",
        "# Copies first 500 files from src to dst then stops.\n",
        "def copy_nfiles(src, dst, max_n):\n",
        "    path = Path(dst)\n",
        "    if not path.exists():\n",
        "        path.mkdir(parents = False, exist_ok = False)\n",
        "    n = 0;\n",
        "    for item in os.listdir(src):\n",
        "        s = os.path.join(src, item)\n",
        "        d = os.path.join(dst, item)\n",
        "        if os.path.isfile(s):\n",
        "            shutil.copy2(s, d)\n",
        "            n += 1\n",
        "        if n == max_n:\n",
        "            print(f'Copy complete, {max_n} files.')\n",
        "            break\n",
        "\n",
        "# - - -\n",
        "# Copy n files from src into dst1, dst2 and dst3.\n",
        "# - - -\n",
        "# Example: copy_nfiles_to_dst123(src,dst1,dst2,dst3, 500)\n",
        "# First 500 files from src into dst1, next 500 into dst2, next 500 into dst3 and stops.\n",
        "def copy_nfiles_to_dst123(src, dst1, dst2, dst3, xsize):\n",
        "    path = Path(dst1)\n",
        "    if not path.exists():\n",
        "        path.mkdir(parents = False, exist_ok = False)\n",
        "    path = Path(dst2)\n",
        "    if not path.exists():\n",
        "        path.mkdir(parents = False, exist_ok = False)\n",
        "    path = Path(dst3)\n",
        "    if not path.exists():\n",
        "        path.mkdir(parents = False, exist_ok = False)\n",
        "    n = 0;\n",
        "    for item in os.listdir(src):\n",
        "\n",
        "        if n >= (xsize*2):\n",
        "            dst = dst1\n",
        "        elif n >= xsize:\n",
        "            dst = dst2\n",
        "        else:\n",
        "            dst = dst3\n",
        "\n",
        "        s = os.path.join(src, item)\n",
        "        d = os.path.join(dst, item)\n",
        "        if os.path.isfile(s):\n",
        "            shutil.copy2(s, d)\n",
        "            n += 1\n",
        "        if n == (xsize*3):\n",
        "            print(f'Copy complete, 3x{xsize} files.')\n",
        "            break\n",
        "\n",
        "def ret_directoryFileCount(dir_path):\n",
        "    file_count = 0\n",
        "    for path in os.listdir(dir_path):\n",
        "        if os.path.isfile(os.path.join(dir_path, path)):\n",
        "            file_count += 1\n",
        "    return file_count\n",
        "\n",
        "\n",
        "#copy_nfiles_to_dst123(\"/content/data/artwork_style_neg_text_v1-5_mse_vae_dpm2SaKarras50_cfg7_n4200\", \"/content/data/art1\", \"/content/data/art2\", \"/content/data/art3\", 1000)\n",
        "#copy_nfiles(\"/content/data/tmp/guy_v1-5_mse_vae_ddim50_cfg7_n4820\", \"/content/data/guy_images_500\", 500)\n",
        "\n",
        "saved_grid_count = 1\n",
        "\n",
        "clear_output(); print('\u001b[1;32mDone! âœ“')\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SKa6d9SXyA1i"
      },
      "outputs": [],
      "source": [
        "#@title Regularization images - class images\n",
        "%cd /content/data/\n",
        "from zipfile import ZipFile\n",
        "\n",
        "#@markdown This cell will download and prepare the regularization images needed for training. Just change the selected archive and reload this cell to change regularization images.\n",
        "regularization_archive = \"man-1000-ddim\" #@param [\"man-1000-ddim\", \"man-4820-ddim\", \"man-394-unsplash\"]\n",
        "\n",
        "REGZIP_ID = ''\n",
        "if regularization_archive == 'man-1000-ddim':\n",
        "    REGZIP_ID = \"guy_v1-5_mse_vae_ddim50_cfg7_n1000\"\n",
        "    !wget https://huggingface.co/datasets/steinhaug/regularization/resolve/main/{REGZIP_ID}.zip -O {REGZIP_ID}.zip\n",
        "elif regularization_archive == 'man-4820-ddim':\n",
        "    REGZIP_ID = \"guy_v1-5_mse_vae_ddim50_cfg7_n4820\"\n",
        "    !wget https://huggingface.co/datasets/ProGamerGov/StableDiffusion-v1-5-Regularization-Images/resolve/main/{REGZIP_ID}.zip -O {REGZIP_ID}.zip\n",
        "elif regularization_archive == 'man-394-unsplash':\n",
        "    REGZIP_ID = \"man_unsplash-394\"\n",
        "    !wget https://huggingface.co/datasets/steinhaug/regularization/resolve/main/{REGZIP_ID}.zip -O {REGZIP_ID}.zip\n",
        "else:\n",
        "    raise Exception(f\"[!] Unknown parameter for regularization_archive.\")\n",
        "\n",
        "if REGZIP_ID != '':\n",
        "    with ZipFile(\"/content/data/\" + REGZIP_ID + \".zip\", 'r') as zObject:\n",
        "        zObject.extractall(path=\"/content/data\")\n",
        "    REGULARIZATION_DIR = \"/content/data/\" + REGZIP_ID\n",
        "    clear_output()\n",
        "    print('\u001b[1;32mRegularization images - unpacked and ready! âœ“')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LCl6bljv108",
        "outputId": "d94cd9f6-69a4-47a7-fe27-4605719283ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;32mDone! âœ“\n"
          ]
        }
      ],
      "source": [
        "from slugify import slugify\n",
        "# Concept setup\n",
        "\n",
        "# 1 - 5\n",
        "SET_NAME = 5\n",
        "\n",
        "instance_token = 'kim steinhaug'\n",
        "class_definition = 'man'\n",
        "\n",
        "concepts_list = [\n",
        "    {\n",
        "        \"instance_prompt\":      \"photo of \" + instance_token + \" \" + class_definition,\n",
        "        \"class_prompt\":         \"photo of a \" + class_definition,\n",
        "        \"instance_data_dir\":    \"/content/data/\" + slugify(instance_token + \"\"),\n",
        "        \"class_data_dir\":       \"/content/data/\" + slugify(instance_token + \"_cls\"),\n",
        "    },\n",
        "    {\n",
        "        \"instance_prompt\":      \"photo of \" + instance_token + \" x1 \" + class_definition,\n",
        "        \"class_prompt\":         \"photo of a \" + class_definition,\n",
        "        \"instance_data_dir\":    \"/content/data/\" + slugify(instance_token + \" x1\"),\n",
        "        \"class_data_dir\":       \"/content/data/\" + slugify(instance_token + \" x1_cls\"),\n",
        "    },\n",
        "    {\n",
        "        \"instance_prompt\":      \"photo of \" + instance_token + \" x2 \" + class_definition,\n",
        "        \"class_prompt\":         \"photo of a \" + class_definition,\n",
        "        \"instance_data_dir\":    \"/content/data/\" + slugify(instance_token + \" x2\"),\n",
        "        \"class_data_dir\":       \"/content/data/\" + slugify(instance_token + \" x2_cls\"),\n",
        "    },\n",
        "    {\n",
        "        \"instance_prompt\":      \"photo of \" + instance_token + \" x3 \" + class_definition,\n",
        "        \"class_prompt\":         \"photo of a \" + class_definition,\n",
        "        \"instance_data_dir\":    \"/content/data/\" + slugify(instance_token + \" x3\"),\n",
        "        \"class_data_dir\":       \"/content/data/\" + slugify(instance_token + \" x3_cls\"),\n",
        "    },\n",
        "    {\n",
        "        \"instance_prompt\":      \"photo of \" + instance_token + \" x4 \" + class_definition,\n",
        "        \"class_prompt\":         \"photo of a \" + class_definition,\n",
        "        \"instance_data_dir\":    \"/content/data/\" + slugify(instance_token + \" x4\"),\n",
        "        \"class_data_dir\":       \"/content/data/\" + slugify(instance_token + \" x4_cls\"),\n",
        "    },\n",
        "]\n",
        "\n",
        "# Create the directory for training images and .json file\n",
        "import json; import os;\n",
        "%cd /content/\n",
        "for c in concepts_list:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(concepts_list, f, indent=4)\n",
        "clear_output(); print('\u001b[1;32mDone! âœ“')\n",
        "\n",
        "def get_last_item_from_path(path_string):\n",
        "    split_items = path_string.split('/')\n",
        "    return split_items[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "32gYIDDR1aCp"
      },
      "outputs": [],
      "source": [
        "#@title Optional - file uploader for training images\n",
        "#@markdown Use file manager on the left panel to upload (drag and drop) to each `instance_data_dir` (it uploads faster), or run this cell.\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "for c in concepts_list:\n",
        "    print(f\"Uploading instance images for `{c['instance_prompt']}`\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = os.path.join(c['instance_data_dir'], filename)\n",
        "        shutil.move(filename, dst_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bBoSWaWxBrkY"
      },
      "outputs": [],
      "source": [
        "#@title Automatic training calibration #alt1\n",
        "#@markdown We calibrate the training setting based on how many training images you have uploaded.<br>\n",
        "#@markdown Regularization images are automatically added from the archive you selected previously.\n",
        "\n",
        "import os; import shutil;\n",
        "\n",
        "for c in concepts_list:\n",
        "    NUM_INSTANCE_IMAGES = ret_directoryFileCount(c[\"instance_data_dir\"]);\n",
        "\n",
        "if NUM_INSTANCE_IMAGES == 0:\n",
        "    raise NameError('No training images uploaded, you need to do this first!')\n",
        "else:\n",
        "    print('[*] Training img count: ', NUM_INSTANCE_IMAGES)\n",
        "    if SET_NAME == 2:\n",
        "        # set2 - 990\n",
        "        n_STEPS_IMG = 90\n",
        "        n_CLASS_IMG = 15\n",
        "        n_STEPS_WRP = 10\n",
        "    elif SET_NAME == 3:\n",
        "        # set3 - 900\n",
        "        n_STEPS_IMG = 100\n",
        "        n_CLASS_IMG = 15\n",
        "        n_STEPS_WRP = 8\n",
        "    elif SET_NAME == 4:\n",
        "        # set4 - 808\n",
        "        n_STEPS_IMG = 101\n",
        "        n_CLASS_IMG = 14\n",
        "        n_STEPS_WRP = 12\n",
        "    elif SET_NAME == 5:\n",
        "        # set5 -\n",
        "        n_STEPS_IMG = 80\n",
        "        n_CLASS_IMG = 25\n",
        "        n_STEPS_WRP = 10\n",
        "    else:\n",
        "        # set1 - 500, 640\n",
        "        n_STEPS_IMG = 80\n",
        "        n_CLASS_IMG = 12\n",
        "        n_STEPS_WRP = 10\n",
        "\n",
        "    LEARNING_RATE = 1e-6\n",
        "    LR_SCHEDULE = \"polynomial\"\n",
        "    NUM_CLASS_IMAGES = NUM_INSTANCE_IMAGES * n_CLASS_IMG\n",
        "    MAX_NUM_STEPS = NUM_INSTANCE_IMAGES * n_STEPS_IMG\n",
        "    LR_WARMUP_STEPS = int(MAX_NUM_STEPS / n_STEPS_WRP)\n",
        "    MAX_TRAIN_STEPS = NUM_INSTANCE_IMAGES * 120\n",
        "\n",
        "    print('[*] SET_NAME: ', SET_NAME)\n",
        "    print(f\"[*] NUM_CLASS_IMAGES {NUM_CLASS_IMAGES}\")\n",
        "    print(f\"[*] MAX_NUM_STEPS {MAX_NUM_STEPS}\")\n",
        "    print(f\"[*] LR_WARMUP_STEPS {LR_WARMUP_STEPS}\")\n",
        "    print(f\"[*] MAX_TRAIN_STEPS {MAX_TRAIN_STEPS}\")\n",
        "\n",
        "    for c in concepts_list:\n",
        "        if os.path.exists(c[\"class_data_dir\"]):\n",
        "            shutil.rmtree(c[\"class_data_dir\"])\n",
        "        os.makedirs(c[\"class_data_dir\"], exist_ok=True)\n",
        "        copy_nfiles(REGULARIZATION_DIR, c[\"class_data_dir\"], NUM_CLASS_IMAGES)\n",
        "        print('[*] Class_data_dir regularization imgs: ', ret_directoryFileCount(c[\"class_data_dir\"]))\n",
        "\n",
        "    print(f'Output: {OUTPUT_DIR}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iPpjQIqEEUH1"
      },
      "outputs": [],
      "source": [
        "#@title Automatic training calibration #alt2\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "def ret_fileCounts(dir_path):\n",
        "    file_count = 0\n",
        "    img_count = 0\n",
        "    txt_count = 0\n",
        "    for path in os.listdir(dir_path):\n",
        "        fileName = path.lower()\n",
        "        if fileName.endswith(('.png', '.jpg', '.jpeg')):\n",
        "            img_count += 1\n",
        "        elif fileName.endswith(('.txt')):\n",
        "            txt_count += 1\n",
        "        if os.path.isfile(os.path.join(dir_path, path)):\n",
        "            file_count += 1\n",
        "\n",
        "    return [file_count, img_count, txt_count]\n",
        "\n",
        "def copy_nExtFiles(src, dst, max_n, ext_a=[], deleteFile=False):\n",
        "    path = Path(dst)\n",
        "    if not path.exists():\n",
        "        path.mkdir(parents = False, exist_ok = False)\n",
        "    n = 0;\n",
        "    for item in os.listdir(src):\n",
        "        s = os.path.join(src, item)\n",
        "        d = os.path.join(dst, item)\n",
        "        if os.path.isfile(s):\n",
        "            if len(ext_a):\n",
        "                if item.endswith(tuple(ext_a)):\n",
        "                    #print('Copied: ' + d)\n",
        "                    if not os.path.isfile(d):\n",
        "                        shutil.copy2(s, d)\n",
        "                        n += 1\n",
        "                        if deleteFile:\n",
        "                            os.remove(s)\n",
        "                    else:\n",
        "                        n += 1\n",
        "            else:\n",
        "                shutil.copy2(s, d)\n",
        "                n += 1\n",
        "        if n == max_n:\n",
        "            break\n",
        "\n",
        "    if len(ext_a):\n",
        "        print(f'Copy complete, {n} files. {tuple(ext_a)}')\n",
        "    else:\n",
        "        print(f'Copy complete, {n} files.')\n",
        "\n",
        "\n",
        "\n",
        "n_STEPS_IMG = 100\n",
        "n_CLASS_IMG = 15\n",
        "n_STEPS_WRP = 8\n",
        "\n",
        "n_save_sample = 5\n",
        "n_save_interval = n_STEPS_IMG * len(concepts_list)\n",
        "\n",
        "tot_instance_img = 0\n",
        "for c in concepts_list:\n",
        "    print(c[\"instance_data_dir\"])\n",
        "    #os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "    #os.makedirs(c[\"class_data_dir\"], exist_ok=True)\n",
        "    x, y, z = ret_fileCounts(c[\"instance_data_dir\"]);\n",
        "    tot_instance_img += y\n",
        "    NUM_CLASS_IMAGES = y * n_CLASS_IMG\n",
        "    copy_nExtFiles(REGULARIZATION_DIR, c[\"class_data_dir\"], NUM_CLASS_IMAGES, ['.jpg','.jpeg','.png'], True)\n",
        "\n",
        "print(f'Total instance imgs: {tot_instance_img}')\n",
        "\n",
        "NUM_INSTANCE_IMAGES = tot_instance_img\n",
        "\n",
        "LEARNING_RATE = 1e-6\n",
        "LR_SCHEDULE = \"polynomial\"\n",
        "MAX_NUM_STEPS = NUM_INSTANCE_IMAGES * n_STEPS_IMG\n",
        "LR_WARMUP_STEPS = int(MAX_NUM_STEPS / n_STEPS_WRP)\n",
        "MAX_TRAIN_STEPS = NUM_INSTANCE_IMAGES * 120\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzZezU3eK8v0"
      },
      "outputs": [],
      "source": [
        "!yes | pip uninstall accelerate\n",
        "!pip install accelerate==0.20.3\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuuy5j2_vh0k"
      },
      "outputs": [],
      "source": [
        "#@title Dreambooth trainer - Lets train the model\n",
        "!python3 train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"main\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"no\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --gradient_checkpointing \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --learning_rate=$LEARNING_RATE \\\n",
        "  --lr_scheduler=$LR_SCHEDULE \\\n",
        "  --lr_warmup_steps=$LR_WARMUP_STEPS \\\n",
        "  --num_class_images=$NUM_CLASS_IMAGES \\\n",
        "  --max_train_steps=$MAX_TRAIN_STEPS \\\n",
        "  --save_interval=$n_save_interval \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --save_sample_prompt=\"photo of kim steinhaug man\" \\\n",
        "  --n_save_sample=$n_save_sample \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "\n",
        "#clear_output()\n",
        "#print('\u001b[1;32mTraining completed! âœ“')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEzOIeGup3If"
      },
      "source": [
        "# Cleanup and finnish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "f2ER2wXKcDPp"
      },
      "outputs": [],
      "source": [
        "#@title GRID MAKER: generate a grid of preview images from the saved checkpoints\n",
        "import os\n",
        "from sys import exit\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "OVERRIDE_OUTPUT_DIR = True #@param {type:\"boolean\"}\n",
        "\n",
        "OVERRIDE_DIR = \"/content/drive/MyDrive/sd_weights/steinhaug-big-realvision50n\" #@param {type:\"string\"}\n",
        "\n",
        "SAVE_GRID_IN_DIR = True #@param {type:\"boolean\"}\n",
        "\n",
        "if OVERRIDE_OUTPUT_DIR:\n",
        "    weights_folder = OVERRIDE_DIR\n",
        "    breaki = 5\n",
        "else:\n",
        "    weights_folder = OUTPUT_DIR\n",
        "    breaki = 99\n",
        "\n",
        "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\" and os.path.isdir(weights_folder + '/' + f)], key=lambda x: int(x))\n",
        "\n",
        "scale = 4\n",
        "\n",
        "row = len(folders)\n",
        "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
        "\n",
        "if col > breaki:\n",
        "    col = breaki\n",
        "\n",
        "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "    folder_path = os.path.join(weights_folder, folder)\n",
        "    image_folder = os.path.join(folder_path, \"samples\")\n",
        "    images = [f for f in os.listdir(image_folder)]\n",
        "    for j, image in enumerate(images):\n",
        "        if row == 1:\n",
        "            currAxes = axes[j]\n",
        "        else:\n",
        "            currAxes = axes[i, j]\n",
        "        if i == 0:\n",
        "            currAxes.set_title(f\"Image {j}\")\n",
        "        if j == 0:\n",
        "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        img = mpimg.imread(image_path)\n",
        "        currAxes.imshow(img, cmap='gray')\n",
        "        currAxes.axis('off')\n",
        "        if j == (breaki - 1):\n",
        "            break;\n",
        "\n",
        "if SAVE_GRID_IN_DIR:\n",
        "    %cd $weights_folder\n",
        "    plt.savefig(f'{saved_grid_count:04d}' + \"_\" + '_grid.png', dpi=72)\n",
        "\n",
        "plt.tight_layout()\n",
        "saved_grid_count += 1\n",
        "%cd /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E_NyVNkEBje0"
      },
      "outputs": [],
      "source": [
        "#@title GRID MAKER - Autodelete folders\n",
        "import os\n",
        "\n",
        "# todo auto select the folders\n",
        "\n",
        "if os.path.isdir(weights_folder):\n",
        "    user_inp_data = input('Do you want to continue? yes or no ')\n",
        "    if user_inp_data == 'yes':\n",
        "        keeper = \"1040\"\n",
        "        keeper2 = \"1111\"\n",
        "\n",
        "        do_delete_all = False\n",
        "\n",
        "        folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\" and os.path.isdir(weights_folder + '/' + f)], key=lambda x: int(x))\n",
        "\n",
        "        for i, folder in enumerate(folders):\n",
        "            if folder == keeper or folder == keeper2:\n",
        "                do_delete_all = True\n",
        "\n",
        "        for i, folder in enumerate(folders):\n",
        "            print(folder)\n",
        "            if folder == keeper or folder == keeper2:\n",
        "                print('We keep this one!')\n",
        "            else:\n",
        "                dirPapth = f'{weights_folder}/' + folder\n",
        "                if do_delete_all:\n",
        "                    print(f'Deleting all files in {dirPapth}')\n",
        "                    !rm -Rf {dirPapth}/scheduler/*\n",
        "                    !rm -Rf {dirPapth}/text_encoder/*\n",
        "                    !rm -Rf {dirPapth}/tokenizer/*\n",
        "                    !rm -Rf {dirPapth}/unet/*\n",
        "                    !rm -Rf {dirPapth}/vae/*\n",
        "                    if os.path.isdir(dirPapth + '/args.json'):\n",
        "                        !rm {dirPapth}/args.json\n",
        "                    if os.path.isdir(dirPapth + '/model_index.json'):\n",
        "                        !rm {dirPapth}/model_index.json\n",
        "\n",
        "        print(f'Done!')\n",
        "    else:\n",
        "        print(f'Make sure you want to do this before continuing')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V8wgU0HN-Kq"
      },
      "source": [
        "## CONVERTERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89Az5NUxOWdy",
        "outputId": "796089ea-96a9-4a48-95c5-d69e28d387c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*] WEIGHTS_DIR=/content/drive/MyDrive/sd_weights/steinhaug-big-realvision50n/6000\n"
          ]
        }
      ],
      "source": [
        "#@markdown Specify the weights directory to use (leave blank for latest)\n",
        "WEIGHTS_DIR = \"/content/drive/MyDrive/sd_weights/steinhaug-big-realvision50n/6000\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "\n",
        "path = Path(WEIGHTS_DIR)\n",
        "if not path.exists():\n",
        "  raise Exception(f\"[*] WEIGHTS_DIR does not exist!\")\n",
        "else:\n",
        "  print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcXzsUyG1aCy",
        "outputId": "167396ac-3c89-47a9-c72d-a127575ab3c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Reshaping encoder.mid.attn_1.q.weight for SD format\n",
            "Reshaping encoder.mid.attn_1.k.weight for SD format\n",
            "Reshaping encoder.mid.attn_1.v.weight for SD format\n",
            "Reshaping encoder.mid.attn_1.proj_out.weight for SD format\n",
            "Reshaping decoder.mid.attn_1.q.weight for SD format\n",
            "Reshaping decoder.mid.attn_1.k.weight for SD format\n",
            "Reshaping decoder.mid.attn_1.v.weight for SD format\n",
            "Reshaping decoder.mid.attn_1.proj_out.weight for SD format\n",
            "[*] Converted ckpt saved at /content/drive/MyDrive/sd_weights/steinhaug-big-realvision50n/6000/model_fp16.ckpt\n"
          ]
        }
      ],
      "source": [
        "#@title CONVERT: diffusers to .ckpt\n",
        "%cd /content/\n",
        "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "\n",
        "if fp16:\n",
        "    ckpt_path = WEIGHTS_DIR + \"/model_fp16.ckpt\"\n",
        "else:\n",
        "    ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
        "\n",
        "half_arg = \"\"\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "p0KvFqPFwW9g"
      },
      "outputs": [],
      "source": [
        "#@title CONVERT: .ckpt to diffusers\n",
        "\n",
        "PATH_CKPT = \"/content/model/realisticVisionV13_v13.ckpt\" #@param {type:\"string\"}\n",
        "PATH_DIFFUSERS = \"/content/diff_model\" #@param {type:\"string\"}\n",
        "\n",
        "## Display device type\n",
        "#from tensorflow.python.client import device_lib\n",
        "#temp = device_lib.list_local_devices()\n",
        "#print(temp[0])\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "path = Path(PATH_CKPT)\n",
        "if not path.is_file():\n",
        "  raise Exception(f\"File not found! Path: {PATH_CKPT}\")\n",
        "\n",
        "path = Path(PATH_DIFFUSERS)\n",
        "if not path.exists():\n",
        "  print(f\"[*] Create directory...\")\n",
        "  path.mkdir(parents = False, exist_ok = False)\n",
        "\n",
        "%cd /content/\n",
        "!python convert_original_stable_diffusion_to_diffusers.py --scheduler_type ddim --checkpoint_path $PATH_CKPT --image_size 512 --prediction_type epsilon --dump_path $PATH_DIFFUSERS --device cpu\n",
        "\n",
        "WEIGHTS_DIR = PATH_DIFFUSERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCyzeaKdv58y"
      },
      "source": [
        "# Diffusion Inference mode: Lets load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW15FjffdTID",
        "outputId": "9e0bb689-911c-4611-db90-ca60c98d06e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;32mDone! âœ“\n"
          ]
        }
      ],
      "source": [
        "#@title Load pipeline, init Diffusers - Stable diffusion inference\n",
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "g_cuda = None\n",
        "\n",
        "from IPython.display import clear_output; clear_output(); print('\u001b[1;32mDone! âœ“')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oIzkltjpVO_f"
      },
      "outputs": [],
      "source": [
        "#@markdown Can set random seed here for reproducibility.\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 1 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)\n",
        "saved_file_count = 1;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6xoHWSsbcS3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Place promt here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFHw44XMhlg6"
      },
      "source": [
        "# Mini Gradio - Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WMCqQ5Tcdsm2"
      },
      "outputs": [],
      "source": [
        "#@markdown Run Gradio UI for generating images.\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        return pipe(\n",
        "                prompt, height=int(height), width=int(width),\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_images_per_prompt=int(num_samples),\n",
        "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "                generator=g_cuda\n",
        "            ).images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=24)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shm-1VYEkV6J",
        "outputId": "14ea4f74-f81e-4de5-bb06-3aee020e75da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "token : steinhaug-big-realvision50n\n",
            "gender: man\n",
            "output: /content/drive/MyDrive/sd_weights/steinhaug-big-realvision50n\n",
            "ckpt : /content/drive/MyDrive/sd_weights/steinhaug-big-realvision50n/6000/model_fp16.ckpt\n"
          ]
        }
      ],
      "source": [
        "YOUR_TOKEN = \"steinhaug-big-realvision50n\"\n",
        "TOKEN_GENDER = \"man\"\n",
        "HUGGINGFACE_TOKEN = \"\"\n",
        "print( 'token : ' + YOUR_TOKEN )\n",
        "print( 'gender: ' + TOKEN_GENDER )\n",
        "print( 'output: ' + OUTPUT_DIR )\n",
        "\n",
        "CKPT_FILENAME = \"kim_steinhaug_fp16.ckpt\"\n",
        "CKPT_FILEPATH = ckpt_path\n",
        "print( 'ckpt : ' + CKPT_FILEPATH )\n",
        "\n",
        "CHECKPOINT_STEPS = \"6000\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0iqP9PM-kHbO"
      },
      "outputs": [],
      "source": [
        "#@markdown **Upload and save your model to Huggingface.co**<br>\n",
        "#@markdown Your diffusers and checkpoint model is already saved in your Google Drive, <br>\n",
        "#@markdown however you can save the model for free at Huggingface.\n",
        "#@markdown This way you can free up your Google Drive space.\n",
        "\n",
        "reload_image = \"\"\"\n",
        "![Reload Setup 3/4 cell](https://raw.githubusercontent.com/steinhaug/stable-diffusion/main/assets/reload-cell-image-s.png)\n",
        "\"\"\"\n",
        "from IPython.display import Markdown\n",
        "def notify_reloader_cell():\n",
        "    try:\n",
        "        variable_value = YOUR_TOKEN\n",
        "    except KeyError:\n",
        "        display(Markdown(reload_image))\n",
        "        raise SystemExit()\n",
        "notify_reloader_cell()\n",
        "\n",
        "# - - - - - - -\n",
        "\n",
        "from slugify import slugify\n",
        "from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "from huggingface_hub import create_repo\n",
        "from IPython.display import display_markdown\n",
        "from IPython.display import clear_output\n",
        "from IPython.utils import capture\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import time\n",
        "import os\n",
        "\n",
        "WPATH = OUTPUT_DIR + f\"/{CHECKPOINT_STEPS}/\"\n",
        "\n",
        "Create_repo = True #@param {type:\"boolean\"}\n",
        "hf_token_write = HUGGINGFACE_TOKEN\n",
        "Name_of_your_concept = YOUR_TOKEN\n",
        "\n",
        "Name_of_your_concept=Name_of_your_concept.replace(\" \",\"-\")\n",
        "hf_token = hf_token_write\n",
        "\n",
        "if not len(hf_token_write):\n",
        "    hf_token = HUGGINGFACE_TOKEN\n",
        "\n",
        "api = HfApi()\n",
        "your_username = api.whoami(token=hf_token)[\"name\"]\n",
        "repo_id = f\"{your_username}/{slugify(Name_of_your_concept)}\"\n",
        "\n",
        "print(\"[1;32mPreparing files...\")\n",
        "\n",
        "UPLOAD_DIR = \"/content/temp\"\n",
        "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "%cd $UPLOAD_DIR\n",
        "\n",
        "#!rm -r safety_checker feature_extractor .git\n",
        "#!rm model_index.json\n",
        "#!git init\n",
        "#!git lfs install --system --skip-repo\n",
        "#!git remote add -f origin  \"https://USER:{hf_token}@huggingface.co/runwayml/stable-diffusion-v1-5\"\n",
        "#!git config core.sparsecheckout true\n",
        "#!echo -e \"feature_extractor\\nsafety_checker\\nmodel_index.json\" > .git/info/sparse-checkout\n",
        "#!git pull origin main\n",
        "#!rm -r .git\n",
        "#%cd /content\n",
        "\n",
        "for c in concepts_list:\n",
        "    tmpInstancePrompt = c[\"instance_prompt\"]\n",
        "    tmpClassPrompt = c[\"class_prompt\"]\n",
        "    tmpImgCount = str(ret_directoryFileCount(c[\"instance_data_dir\"]))\n",
        "\n",
        "readme_text = f'''---\n",
        "license: mit\n",
        "---\n",
        "## Dreambooth for people in a hurry - Colab edition\n",
        "\n",
        "The Stable-Diffusion-v1-5 checkpoint is used as base model, and trained with custom concept.\n",
        "\n",
        "### Concept info\n",
        "\n",
        "Your full token: <b>{YOUR_TOKEN} {TOKEN_GENDER}</b>\n",
        "Example prompt: Professional headshot photo of {YOUR_TOKEN} {TOKEN_GENDER} as a magician, Tiffen Digital Diffusion / FX, 100mm\n",
        "\n",
        "Instance prompt: {tmpInstancePrompt}\n",
        "Class prompt: {tmpClassPrompt}\n",
        "\n",
        "### Model info\n",
        "\n",
        "Training images: {tmpImgCount}\n",
        "Regularization images: 50\n",
        "Model type: Diffusers, Checkpoint\n",
        "\n",
        "training_steps: 1000\n",
        "lr_scheduler: constant\n",
        "lr_warmup_steps: 0\n",
        "learning rate: 1e-6\n",
        "mixed_precision: fp16\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "if Create_repo:\n",
        "    create_repo(repo_id,private=True, token=hf_token)\n",
        "\n",
        "    readme_file = open(\"README.md\", \"w\")\n",
        "    readme_file.write(readme_text)\n",
        "    readme_file.close()\n",
        "    operations = [\n",
        "      CommitOperationAdd(path_in_repo=\"README.md\", path_or_fileobj=\"README.md\")\n",
        "    ]\n",
        "    api.create_commit(\n",
        "      repo_id=repo_id,\n",
        "      operations=operations,\n",
        "      commit_message=f\"Init new diffusers repo\",\n",
        "      token=hf_token\n",
        "    )\n",
        "\n",
        "clear_output()\n",
        "print('Repo created, ready for file upload...')\n",
        "\n",
        "#\n",
        "# Part 2\n",
        "#\n",
        "\n",
        "from IPython.display import clear_output; clear_output();\n",
        "from slugify import slugify\n",
        "from huggingface_hub import HfApi, HfFolder, CommitOperationAdd\n",
        "from huggingface_hub import create_repo\n",
        "from IPython.display import display_markdown\n",
        "from IPython.display import clear_output\n",
        "from IPython.utils import capture\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import time\n",
        "import os\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "\n",
        "operations = [\n",
        "    CommitOperationAdd(path_in_repo=CKPT_FILENAME,path_or_fileobj=CKPT_FILEPATH)\n",
        "]\n",
        "api.create_commit(\n",
        "    repo_id=repo_id,\n",
        "    operations=operations,\n",
        "    commit_message=f\"Added checkpoint model.\",\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "if os.path.isdir(WPATH + 'feature_extractor'):\n",
        "    api.upload_folder(\n",
        "        folder_path = WPATH + 'feature_extractor',\n",
        "        path_in_repo=\"feature_extractor\",\n",
        "        repo_id=repo_id, token=hf_token\n",
        "    )\n",
        "\n",
        "api.upload_folder(\n",
        "    folder_path=WPATH + 'scheduler',\n",
        "    path_in_repo=\"scheduler\",\n",
        "    repo_id=repo_id, token=hf_token\n",
        ")\n",
        "api.upload_folder(\n",
        "    folder_path=WPATH + 'text_encoder',\n",
        "    path_in_repo=\"text_encoder\",\n",
        "    repo_id=repo_id, token=hf_token\n",
        ")\n",
        "api.upload_folder(\n",
        "    folder_path=WPATH + 'tokenizer',\n",
        "    path_in_repo=\"tokenizer\",\n",
        "    repo_id=repo_id, token=hf_token\n",
        ")\n",
        "api.upload_folder(\n",
        "    folder_path=WPATH + 'unet',\n",
        "    path_in_repo=\"unet\",\n",
        "    repo_id=repo_id, token=hf_token\n",
        ")\n",
        "api.upload_folder(\n",
        "    folder_path=WPATH + 'vae',\n",
        "    path_in_repo=\"vae\",\n",
        "    repo_id=repo_id, token=hf_token\n",
        ")\n",
        "api.upload_folder(\n",
        "    folder_path=WPATH + 'unet',\n",
        "    path_in_repo=\"unet\",\n",
        "    repo_id=repo_id, token=hf_token\n",
        ")\n",
        "api.upload_folder(\n",
        "    folder_path=WPATH + 'vae',\n",
        "    path_in_repo=\"vae\",\n",
        "    repo_id=repo_id, token=hf_token\n",
        ")\n",
        "\n",
        "MODEL2 = WPATH + 'args.json'\n",
        "MODEL3 = WPATH + 'model_index.json'\n",
        "operations = [\n",
        "    CommitOperationAdd(path_in_repo=\"args.json\",path_or_fileobj=MODEL2),\n",
        "    CommitOperationAdd(path_in_repo=\"model_index.json\",path_or_fileobj=MODEL3)\n",
        "]\n",
        "api.create_commit(\n",
        "    repo_id=repo_id,\n",
        "    operations=operations,\n",
        "    commit_message=f\"Added my diffusers model, {CHECKPOINT_STEPS} steps.\",\n",
        "    token=hf_token\n",
        ")\n",
        "\n",
        "# Upload training images\n",
        "\n",
        "if len(concepts_list) > 1:\n",
        "    multi_upl = True\n",
        "else:\n",
        "    multi_upl = False\n",
        "print('Uploading training and regularization images...')\n",
        "for c in concepts_list:\n",
        "    if os.path.isdir(c[\"instance_data_dir\"]):\n",
        "        if multi_upl:\n",
        "            repo_dest_path = \"/dataset/\" + get_last_item_from_path(c[\"instance_data_dir\"])\n",
        "        else:\n",
        "            repo_dest_path = \"/dataset/training-images/\"\n",
        "        api.upload_folder(\n",
        "            folder_path=c[\"instance_data_dir\"],\n",
        "            path_in_repo=repo_dest_path,\n",
        "            repo_id=repo_id, token=hf_token\n",
        "        )\n",
        "    if os.path.isdir(c[\"class_data_dir\"]):\n",
        "        if multi_upl:\n",
        "            repo_dest_path = \"/dataset/\" + get_last_item_from_path(c[\"class_data_dir\"])\n",
        "        else:\n",
        "            repo_dest_path = \"/dataset/regularization-images/\"\n",
        "        api.upload_folder(\n",
        "            folder_path=c[\"class_data_dir\"],\n",
        "            path_in_repo=repo_dest_path,\n",
        "            repo_id=repo_id, token=hf_token\n",
        "        )\n",
        "\n",
        "# Uploading generated images\n",
        "\n",
        "if os.path.exists('/content/drive/MyDrive/AutoAI-Images'):\n",
        "    file_count = ret_directoryFileCount('/content/drive/MyDrive/AutoAI-Images')\n",
        "    if file_count:\n",
        "        api.upload_folder(\n",
        "            folder_path='/content/drive/MyDrive/AutoAI-Images',\n",
        "            path_in_repo=\"sample-images/AutoAI\",\n",
        "            repo_id=repo_id, token=hf_token\n",
        "        )\n",
        "\n",
        "clear_output();\n",
        "print('[1;32mModel succesfully uploaded to Huggingface. âœ“')\n",
        "display_markdown(f'''## Model link: [huggingface repository](https://huggingface.co/{repo_id})\n",
        "''', raw=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5V8wgU0HN-Kq",
        "BCyzeaKdv58y",
        "PFHw44XMhlg6"
      ],
      "gpuClass": "premium",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "2.7.16 (default, Oct 10 2019, 22:02:15) \n[GCC 8.3.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
